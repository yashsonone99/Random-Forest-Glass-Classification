# Random-Forest-Glass-Classification
â€œPredicting glass types using Random Forest â€” from data cleaning, EDA, to ensemble learning and hyperparameter tuning ğŸŒ²ğŸ“Šâ€
# ğŸŒ² Random Forest Classification on Glass Dataset  

This project implements a **Random Forest Classifier** to predict the **type of glass** based on its chemical composition.  
It covers the entire pipeline â€” from **data cleaning and EDA** to **model training, tuning, and ensemble comparison** â€” providing a hands-on understanding of classification and ensemble learning techniques in machine learning.  

---

## ğŸ¯ Objectives  
- Perform detailed **data cleaning** and handle missing or inconsistent data  
- Conduct **Exploratory Data Analysis (EDA)** using histograms, boxplots, and correlation heatmaps  
- Build and evaluate a **Random Forest Classifier** for multi-class prediction  
- Perform **hyperparameter tuning** using GridSearchCV  
- Compare performance with **Bagging**, **AdaBoost**, and **Gradient Boosting** models  
- Save the **best performing model** for deployment  

---

## âš™ï¸ Technologies Used  
- **Python** ğŸ  
- **Pandas**, **NumPy** â€” Data manipulation and preprocessing  
- **Matplotlib**, **Seaborn** â€” Data visualization  
- **Scikit-learn** â€” Model building, tuning, and evaluation  
- **Joblib** â€” Model saving  

---

## ğŸ“ Files Included  
| File Name | Description |
|------------|-------------|
| `Random_forest.ipynb` | Complete notebook containing code and visualizations |
| `glass.csv` | Input dataset containing chemical composition of glass samples |
| `best_random_forest_glass.pkl` | Saved model after hyperparameter tuning |
| `README.md` | Project documentation |

---

## âš™ï¸ Steps Involved  

### 1ï¸âƒ£ Data Loading & Cleaning  
- Loaded `glass.csv` dataset  
- Removed missing values and duplicates  
- Ensured all numeric features were correctly typed  

### 2ï¸âƒ£ Exploratory Data Analysis (EDA)  
- Visualized data distributions using histograms and boxplots  
- Created a **correlation heatmap** for feature relationships  
- Checked **target variable balance** (Glass Type)

### 3ï¸âƒ£ Data Preprocessing  
- Scaled features using **StandardScaler**  
- Split data into training and testing sets (75:25 ratio)

### 4ï¸âƒ£ Model Building  
- Built a **baseline Random Forest model**  
- Evaluated using:
  - Accuracy  
  - Precision  
  - Recall  
  - F1-Score  
  - Confusion Matrix  

### 5ï¸âƒ£ Hyperparameter Tuning  
- Applied **GridSearchCV** to optimize:
  - `n_estimators`, `max_depth`, `min_samples_split`, `min_samples_leaf`  
- Selected best performing model and re-evaluated on test data  

### 6ï¸âƒ£ Ensemble Learning Comparison  
Compared Random Forest with:
- **Bagging Classifier** (with Random Forest base)  
- **AdaBoost Classifier**  
- **Gradient Boosting Classifier**  

Displayed comparative performance with accuracy metrics and confusion matrices.

### 7ï¸âƒ£ Model Saving  
- Exported the tuned Random Forest model as  
  ```python
  joblib.dump(best_rf, "best_random_forest_glass.pkl")
